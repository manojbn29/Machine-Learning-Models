{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specialized-luxury",
   "metadata": {},
   "source": [
    "# **Naive Bayes Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "agreed-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "underlying-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding = 'iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "primary-finish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "relevant-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "colored-american",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-gospel",
   "metadata": {},
   "source": [
    "### Steps to clean the text for classification task\n",
    "\n",
    "1. Remove all the punctuation marks\n",
    "2. Lower case all the letters\n",
    "3. Remove all the stopwords\n",
    "4. Lemmetize the data\n",
    "4. find a way to convert the text data to numerical data\n",
    "5. pass it to the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electrical-presentation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The message sent is askin for  &lt;#&gt; dollars. Shoul i pay  &lt;#&gt;  or  &lt;#&gt; ?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['v2'][435]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "historical-research",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loaded-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) removing puncuation (i.e. special characters)\n",
    "\n",
    "df['v2'] = df['v2'].apply(lambda x: ''.join([i for i in x if i not in string.punctuation]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "known-emphasis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The message sent is askin for  ltgt dollars Shoul i pay  ltgt  or  ltgt '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v2'][435]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "maritime-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) making all as lower case\n",
    "\n",
    "df['v2'] = df['v2'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "third-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"isn't\", 'do', 'had', 'while', 'don', 'your', 'nor', \"couldn't\", 'shan', 'you', 'that', 'has', 'mustn', 'once', 's', \"don't\", \"you've\", 'these', 'through', 'to', 've', \"should've\", 'ourselves', 'been', 'at', 'm', 'haven', \"hadn't\", 'be', 'ma', 'as', \"hasn't\", \"didn't\", 'an', 'yourselves', 'needn', 'ours', 'other', 'her', \"haven't\", \"shan't\", 'shouldn', 'into', 'any', 'mightn', 'there', 'are', 'y', 'where', 'both', 'aren', 'were', 'won', \"she's\", 'down', 'themselves', 'again', 'wasn', 'about', 'out', \"shouldn't\", 'ain', 'doesn', 'being', 'd', 'hers', 'its', 'above', 'll', 'so', 'under', 'or', 'have', 'off', 'and', 'him', 'with', 'theirs', 'then', 'we', 'was', 'weren', \"you'd\", 'such', 'against', 'couldn', 'but', 'more', 'they', 'up', 'i', 'their', 'no', 'himself', 'our', 'who', \"mightn't\", 'his', 'should', 'a', 'only', 'of', 'o', 'hasn', 'here', 'further', 'when', 'than', \"needn't\", \"weren't\", 'this', 'over', 'before', 'all', 'it', 'did', 'some', 'because', 'own', 'myself', 'yourself', \"you're\", 'now', 'my', 'wouldn', 'isn', 'if', 'didn', \"wasn't\", 'which', \"that'll\", 'not', 'itself', 'how', 'does', \"mustn't\", 'them', 'herself', 'same', 'until', 'will', 'the', 'am', 'those', 'during', 'below', 'on', 'yours', 'whom', 'hadn', 'most', \"it's\", 'why', 'just', \"doesn't\", 'she', 'having', \"wouldn't\", 'what', 'in', \"won't\", 'each', 't', 'doing', 'after', 'he', 'is', 'by', 'can', 're', 'very', \"you'll\", 'me', 'few', 'for', 'from', 'too', 'between', \"aren't\"}\n"
     ]
    }
   ],
   "source": [
    "# remove the stopwords (we can use nltk library or spacu library)\n",
    "\n",
    "# Stop words from nltk library\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handmade-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'towards', 'next', 'us', 'everything', 'noone', 'done', 'various', 'might', 'alone', 'these', 'to', 'regarding', 'whereas', 'latter', 'mostly', 'ours', 'her', 'twenty', 'nobody', 'part', 'down', 'themselves', 'out', 'under', 'within', 'him', 'we', 'toward', 'against', 'mine', 'they', 'up', 'no', 'himself', 'anything', 'a', 'together', 'everywhere', 'than', '‘ll', 'fifty', 'name', 'this', 'meanwhile', 'back', 'all', 'did', 'also', 'because', 'seeming', 'my', 'across', 'twelve', 'full', 'except', 'six', 'would', 'thereby', 'herself', 'them', 'those', 'below', 'whither', 'yours', 'still', 'everyone', 'n’t', 'seems', 'fifteen', 'serious', 'n‘t', 'doing', 'perhaps', 'side', 'from', 'too', 'between', \"'ve\", 'had', 'while', 'your', 'less', 'nor', 'becoming', 'put', 'sixty', 'has', 'anywhere', 'therein', 'somehow', 'third', 'least', 'via', 'at', 'be', 'though', 'as', 'ca', 'amount', 'never', 'an', 'yourselves', '’ll', 'else', 'first', 'where', 'again', 'although', 'whereupon', 'without', 'anyhow', 'have', 'off', 'upon', 'and', 'with', 'somewhere', 'then', 'more', 'indeed', 'thence', 'three', 'sometimes', 'who', 'wherein', 'should', 'must', 'of', 'four', 'nowhere', 'when', 'before', 'take', 'it', 'several', 'myself', 'otherwise', 'if', 'beyond', 'front', 'wherever', 'how', 'move', 'same', 'something', 'the', 'whose', 'another', 'whoever', 'whom', 'besides', 'why', 'others', 'throughout', 'one', 'what', 'is', 'by', 'seemed', 'ten', 'could', 'make', 'thru', 'last', 'herein', 'rather', 'once', 'five', '‘ve', 'empty', \"n't\", 'other', 'eight', 'forty', 'into', 'are', 'per', 'both', 'namely', '‘re', '’m', 'whereby', \"'d\", 'go', 'among', 'hers', 'above', 'quite', 'or', 'eleven', 'thus', 'was', 'such', 'neither', 'their', '’re', 'top', 'nevertheless', 'here', 'since', 'over', 'some', 'none', 'yourself', 'now', 'thereafter', 'former', 'hereby', 'due', 'which', 'itself', 'does', 'ever', 'amongst', 'seem', 'show', 'will', 'latterly', 'sometime', 'whatever', 'am', 'whether', 'see', 'during', 'may', 'whence', '’ve', \"'m\", 'on', 'get', 'afterwards', 'much', 'she', 'hereafter', 'in', 'hundred', 'really', 'can', \"'ll\", 're', 'whereafter', 'yet', 'became', 'very', \"'re\", 'become', 'few', 'for', 'formerly', 'many', 'do', 'around', 'you', 'either', 'that', 'thereupon', 'through', 'ourselves', 'along', 'been', '‘m', 'hence', '‘d', 'hereupon', 'call', 'unless', 'say', 'any', 'there', 'were', 'please', 'elsewhere', 'beforehand', 'bottom', 'about', 'every', '’d', 'being', 'whenever', 'cannot', '’s', 'its', 'so', 'therefore', 'but', 'i', 'give', 'our', 'his', 'already', 'often', 'only', 'further', '‘s', 'becomes', 'own', 'whole', 'always', 'however', 'used', 'well', 'not', \"'s\", 'keep', 'anyone', 'until', 'two', 'moreover', 'nine', 'enough', 'behind', 'most', 'just', 'almost', 'even', 'made', 'nothing', 'each', 'using', 'after', 'he', 'onto', 'me', 'someone', 'anyway', 'beside'}\n"
     ]
    }
   ],
   "source": [
    "# Stop words from spacy library\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "robust-adoption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using the spacy library because it is more advanced and have more stopwords compared to nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dental-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v2'] = df['v2'].apply(lambda x: ' '.join([i for i in x.split() if i not in STOP_WORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "assisted-charger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'free entry 2 wkly comp win fa cup final tkts 21st 2005 text fa 87121 receive entry questionstd txt ratetcs apply 08452810075over18s'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['v2'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "concerned-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3)Limmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "regional-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manoj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "exotic-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fifty-excess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Just some examples how lemmitizer is working\n",
    "\n",
    "lemmatizer.lemmatize('going', wordnet.VERB)\n",
    "\n",
    "# lemmatizer.lemmatize('working', wordnet.VERB)\n",
    "\n",
    "# lemmatizer.lemmatize('swim', wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "neural-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\manoj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'), ('are', 'VBP'), ('going', 'VBG')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(['we', 'are','going'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "hollywood-capability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'doing', 'good']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'We are doing good'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sacred-blair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'PRP'), ('are', 'VBP'), ('doing', 'VBG'), ('good', 'JJ')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag('we are doing good'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "utility-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['v2'] = df['v2'].apply(lambda x: ' '.join([lemmatizer.lemmatize(i[0], get_wordnet_pos(i[1])) for i in nltk.pos_tag(x.split())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "centered-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_wordnet_pos(i[1] is mapping with function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "civil-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(max_features = 4000)\n",
    "\n",
    "tfidf_vectors = vec.fit_transform(df['v2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "endless-justice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x4000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37741 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "vulnerable-radiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02073162414</th>\n",
       "      <th>020903</th>\n",
       "      <th>021</th>\n",
       "      <th>050703</th>\n",
       "      <th>0578</th>\n",
       "      <th>071104</th>\n",
       "      <th>...</th>\n",
       "      <th>yun</th>\n",
       "      <th>yunny</th>\n",
       "      <th>yuo</th>\n",
       "      <th>yup</th>\n",
       "      <th>zed</th>\n",
       "      <th>zoe</th>\n",
       "      <th>åð</th>\n",
       "      <th>ìll</th>\n",
       "      <th>ìï</th>\n",
       "      <th>ûò</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   008704050406  01223585334  020603  0207  02073162414  020903  021  050703  \\\n",
       "0           0.0          0.0     0.0   0.0          0.0     0.0  0.0     0.0   \n",
       "1           0.0          0.0     0.0   0.0          0.0     0.0  0.0     0.0   \n",
       "2           0.0          0.0     0.0   0.0          0.0     0.0  0.0     0.0   \n",
       "3           0.0          0.0     0.0   0.0          0.0     0.0  0.0     0.0   \n",
       "4           0.0          0.0     0.0   0.0          0.0     0.0  0.0     0.0   \n",
       "\n",
       "   0578  071104  ...  yun  yunny  yuo  yup  zed  zoe   åð  ìll   ìï   ûò  \n",
       "0   0.0     0.0  ...  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1   0.0     0.0  ...  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2   0.0     0.0  ...  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3   0.0     0.0  ...  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4   0.0     0.0  ...  0.0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 4000 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see how the data is there in tfidf_vectors \n",
    "\n",
    "new_df = pd.DataFrame(tfidf_vectors.toarray(), columns = vec.get_feature_names())\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "clinical-income",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 4000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-supervisor",
   "metadata": {
    "tags": []
   },
   "source": [
    "Curse of dimensionality will not happen\n",
    "\n",
    "Note: If we directly apply one hot encoding to any column then it will result in curse of dimensionality so we are using \n",
    "\n",
    "1) reducing the stop words\n",
    "2) using limmetization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-sandwich",
   "metadata": {},
   "source": [
    "# Actual model (i.e Navies Bayes Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "accurate-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the y column i.e. making ham to 0 and spam to 1\n",
    "\n",
    "y = df['v1'] = df['v1'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "significant-neutral",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "least-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vectors, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "wrapped-bacteria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "round-wages",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8771300448430494\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_model.predict(X_test.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score = accuracy_score(y_pred, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "unique-begin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_model1 = MultinomialNB()\n",
    "nb_model1.fit(X_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "indoor-indiana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9766816143497757\n"
     ]
    }
   ],
   "source": [
    "y_pred_m = nb_model1.predict(X_test.toarray())\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "score1 = accuracy_score(y_pred_m, y_test)\n",
    "print(score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-arrival",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
